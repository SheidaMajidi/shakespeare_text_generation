{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import optuna\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer, Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
    "with open(path_to_file, 'r') as file:\n",
    "    text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(set(text))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "# encode the text\n",
    "encoded_text = np.array([char_to_idx[ch] for ch in text])\n",
    "\n",
    "# Define parameters\n",
    "seq_length = 100\n",
    "batch_size = 64\n",
    "\n",
    "def get_batches(encoded_text, batch_size, seq_length):\n",
    "    total_length = len(encoded_text)\n",
    "    num_batches = total_length // (batch_size * seq_length)\n",
    "\n",
    "    encoded_text = encoded_text[:num_batches * batch_size * seq_length]\n",
    "    encoded_text = encoded_text.reshape((batch_size, -1))\n",
    "\n",
    "    for i in range(0, encoded_text.shape[1], seq_length):\n",
    "        x = encoded_text[:, i:i+seq_length]\n",
    "        y = np.zeros_like(x)\n",
    "        if i+seq_length < encoded_text.shape[1]:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], encoded_text[:, i+seq_length]\n",
    "        else:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], encoded_text[:, 0]\n",
    "        yield torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, dropout=0.5):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = out.reshape(out.size(0)*out.size(1), self.hidden_size)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.num_layers, batch_size, self.hidden_size).zero_(),\n",
    "                weight.new(self.num_layers, batch_size, self.hidden_size).zero_())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20, Step: 0, Loss: 4.169116497039795\n",
      "Epoch: 1/20, Step: 100, Loss: 2.100273370742798\n",
      "Epoch: 2/20, Step: 0, Loss: 1.9322454929351807\n",
      "Epoch: 2/20, Step: 100, Loss: 1.7591819763183594\n",
      "Epoch: 3/20, Step: 0, Loss: 1.7280583381652832\n",
      "Epoch: 3/20, Step: 100, Loss: 1.6501561403274536\n",
      "Epoch: 4/20, Step: 0, Loss: 1.6433299779891968\n",
      "Epoch: 4/20, Step: 100, Loss: 1.5912357568740845\n",
      "Epoch: 5/20, Step: 0, Loss: 1.5942225456237793\n",
      "Epoch: 5/20, Step: 100, Loss: 1.5429699420928955\n",
      "Epoch: 6/20, Step: 0, Loss: 1.5584497451782227\n",
      "Epoch: 6/20, Step: 100, Loss: 1.5137763023376465\n",
      "Epoch: 7/20, Step: 0, Loss: 1.5180014371871948\n",
      "Epoch: 7/20, Step: 100, Loss: 1.491997241973877\n",
      "Epoch: 8/20, Step: 0, Loss: 1.505213975906372\n",
      "Epoch: 8/20, Step: 100, Loss: 1.4864345788955688\n",
      "Epoch: 9/20, Step: 0, Loss: 1.5005873441696167\n",
      "Epoch: 9/20, Step: 100, Loss: 1.4643430709838867\n",
      "Epoch: 10/20, Step: 0, Loss: 1.4700899124145508\n",
      "Epoch: 10/20, Step: 100, Loss: 1.4506616592407227\n",
      "Epoch: 11/20, Step: 0, Loss: 1.4453233480453491\n",
      "Epoch: 11/20, Step: 100, Loss: 1.4399529695510864\n",
      "Epoch: 12/20, Step: 0, Loss: 1.4506957530975342\n",
      "Epoch: 12/20, Step: 100, Loss: 1.4319807291030884\n",
      "Epoch: 13/20, Step: 0, Loss: 1.4392296075820923\n",
      "Epoch: 13/20, Step: 100, Loss: 1.4174920320510864\n",
      "Epoch: 14/20, Step: 0, Loss: 1.4126238822937012\n",
      "Epoch: 14/20, Step: 100, Loss: 1.4065333604812622\n",
      "Epoch: 15/20, Step: 0, Loss: 1.4055639505386353\n",
      "Epoch: 15/20, Step: 100, Loss: 1.408659815788269\n",
      "Epoch: 16/20, Step: 0, Loss: 1.4049984216690063\n",
      "Epoch: 16/20, Step: 100, Loss: 1.3931899070739746\n",
      "Epoch: 17/20, Step: 0, Loss: 1.3978837728500366\n",
      "Epoch: 17/20, Step: 100, Loss: 1.3847233057022095\n",
      "Epoch: 18/20, Step: 0, Loss: 1.388475775718689\n",
      "Epoch: 18/20, Step: 100, Loss: 1.3818320035934448\n",
      "Epoch: 19/20, Step: 0, Loss: 1.380824089050293\n",
      "Epoch: 19/20, Step: 100, Loss: 1.3646063804626465\n",
      "Epoch: 20/20, Step: 0, Loss: 1.3769818544387817\n",
      "Epoch: 20/20, Step: 100, Loss: 1.3681929111480713\n"
     ]
    }
   ],
   "source": [
    "# hyperparameters\n",
    "vocab_size = len(chars)\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_epochs = 20\n",
    "learning_rate = 0.002\n",
    "\n",
    "# GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# model, optimizer, and loss function\n",
    "model = CharRNN(vocab_size, hidden_size, num_layers).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# training loop\n",
    "for epoch in range(num_epochs):\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    hidden = tuple([each.data.to(device) for each in hidden])\n",
    "\n",
    "    for i, (x, y) in enumerate(get_batches(encoded_text, batch_size, seq_length)):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(x, hidden)\n",
    "        loss = criterion(output, y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Step: {i}, Loss: {loss.item()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_str, length, hidden_size, num_layers):\n",
    "    model.eval()\n",
    "    chars = [char_to_idx[ch] for ch in start_str]\n",
    "    input = torch.tensor(chars, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    hidden = model.init_hidden(1)\n",
    "    hidden = tuple([each.data.to(device) for each in hidden])\n",
    "\n",
    "    generated_text = start_str\n",
    "    for i in range(length):\n",
    "        output, hidden = model(input, hidden)\n",
    "        p = torch.nn.functional.softmax(output, dim=1).data\n",
    "        top_ch = torch.multinomial(p, 1)[-1]\n",
    "        char = idx_to_char[top_ch.item()]\n",
    "        generated_text += char\n",
    "        input = torch.tensor([top_ch], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be, or not to be.\n",
      "Romeo, to grant the king knows not to my time.\n",
      "Sirrat that the sun a lord that making, last\n",
      "So Cap\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(model, start_str=\"To be, or not to be\", length=100, hidden_size=hidden_size, num_layers=num_layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, data, seq_length, batch_size):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    hidden = tuple([each.data.to(device) for each in hidden])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output, hidden = model(x, hidden)\n",
    "            loss = criterion(output, y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / (len(data) / (batch_size * seq_length))\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20, Step: 0, Loss: 1.3977503776550293\n",
      "Epoch: 1/20, Step: 100, Loss: 1.2753419876098633\n",
      "Epoch: 1, Validation Perplexity: 3.5572266386786717\n",
      "To be, or not to be,\n",
      "When my masters doth show the such any coover thee:\n",
      "Here is feel, when we were despair:\n",
      "He was alo\n",
      "Epoch: 2/20, Step: 0, Loss: 1.377517580986023\n",
      "Epoch: 2/20, Step: 100, Loss: 1.263879418373108\n",
      "Epoch: 2, Validation Perplexity: 3.582075532662698\n",
      "To be, or not to be\n",
      "rise more seven frowned in thyself but bring.\n",
      "O bloody daughter, his severe, my lord;\n",
      "And yet I may\n",
      "Epoch: 3/20, Step: 0, Loss: 1.367159128189087\n",
      "Epoch: 3/20, Step: 100, Loss: 1.2707512378692627\n",
      "Epoch: 3, Validation Perplexity: 3.6066422324131526\n",
      "To be, or not to be them.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "I women may not made the pleasant--\n",
      "\n",
      "SICINIUS:\n",
      "Yes, Catesby?\n",
      "\n",
      "EXETER:\n",
      "O, now\n",
      "Epoch: 4/20, Step: 0, Loss: 1.3637961149215698\n",
      "Epoch: 4/20, Step: 100, Loss: 1.2625248432159424\n",
      "Epoch: 4, Validation Perplexity: 3.6301358618108712\n",
      "To be, or not to be\n",
      "disposition; if which illy upon thee,\n",
      "Cherish I are signior did before to-day;\n",
      "Fein them, they reep\n",
      "Epoch: 5/20, Step: 0, Loss: 1.3577165603637695\n",
      "Epoch: 5/20, Step: 100, Loss: 1.26268470287323\n",
      "Epoch: 5, Validation Perplexity: 3.632852868126224\n",
      "To be, or not to be\n",
      "then run of like; who did half at his matter,\n",
      "How they were to'clow, madam Coriole\n",
      "To ancient nire \n",
      "Epoch: 6/20, Step: 0, Loss: 1.3411303758621216\n",
      "Epoch: 6/20, Step: 100, Loss: 1.2489432096481323\n",
      "Epoch: 6, Validation Perplexity: 3.6513841527747974\n",
      "To be, or not to below,\n",
      "My monst rose over for but walk, nor presently.\n",
      "\n",
      "HERMIONE:\n",
      "Swell the County, what's the which s\n",
      "Epoch: 7/20, Step: 0, Loss: 1.337262749671936\n",
      "Epoch: 7/20, Step: 100, Loss: 1.254073143005371\n",
      "Epoch: 7, Validation Perplexity: 3.660782304337761\n",
      "To be, or not to be;\n",
      "And live Clarence, a gentleman, in the chairs:\n",
      "And much companion'd with my went from heaven,\n",
      "As g\n",
      "Epoch: 8/20, Step: 0, Loss: 1.321122407913208\n",
      "Epoch: 8/20, Step: 100, Loss: 1.2503525018692017\n",
      "Epoch: 8, Validation Perplexity: 3.6838382430198435\n",
      "To be, or not to bear-suffery;\n",
      "It should death hither the world doth sensely sean\n",
      "to hear perease of his honour did unw\n",
      "Epoch: 9/20, Step: 0, Loss: 1.3290706872940063\n",
      "Epoch: 9/20, Step: 100, Loss: 1.2355237007141113\n",
      "Epoch: 9, Validation Perplexity: 3.6875688533232927\n",
      "To be, or not to be\n",
      "door fair scold.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "The noble call'd so books a consider'd\n",
      "To fear 'pick'd: being a h\n",
      "Epoch: 10/20, Step: 0, Loss: 1.316953420639038\n",
      "Epoch: 10/20, Step: 100, Loss: 1.2423361539840698\n",
      "Epoch: 10, Validation Perplexity: 3.6813339193907337\n",
      "To be, or not to be\n",
      "made him, and he were well thou art;\n",
      "And thou hast revenge, as we have defied him.\n",
      "\n",
      "GLOUCESTER:\n",
      "Why\n",
      "Epoch: 11/20, Step: 0, Loss: 1.3101612329483032\n",
      "Epoch: 11/20, Step: 100, Loss: 1.235568881034851\n",
      "Epoch: 11, Validation Perplexity: 3.7012317629428586\n",
      "To be, or not to be,\n",
      "Whom I cannot? A ghostly sovereign,\n",
      "Either embrace my clothes: leeven nor deer\n",
      "Shall be rothens to\n",
      "Epoch: 12/20, Step: 0, Loss: 1.3077332973480225\n",
      "Epoch: 12/20, Step: 100, Loss: 1.2264846563339233\n",
      "Epoch: 12, Validation Perplexity: 3.7103585498953446\n",
      "To be, or not to be,\n",
      "For whise ministerning counsel of his title's leave.\n",
      "If he hath scarned honour.\n",
      "\n",
      "BRUTUS:\n",
      "Grow and \n",
      "Epoch: 13/20, Step: 0, Loss: 1.3127610683441162\n",
      "Epoch: 13/20, Step: 100, Loss: 1.2380863428115845\n",
      "Epoch: 13, Validation Perplexity: 3.7182333588641425\n",
      "To be, or not to be.\n",
      "\n",
      "HORTENSIO:\n",
      "Lustier, as even to be; for you are.\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Ar with there, not, good father,\n",
      "Epoch: 14/20, Step: 0, Loss: 1.2839057445526123\n",
      "Epoch: 14/20, Step: 100, Loss: 1.2208073139190674\n",
      "Epoch: 14, Validation Perplexity: 3.7170847317131552\n",
      "To be, or not to bear\n",
      "helpens, I love thy consibts will do him be seen;\n",
      "Whom here to do the devil and be free,\n",
      "Or let u\n",
      "Epoch: 15/20, Step: 0, Loss: 1.3033473491668701\n",
      "Epoch: 15/20, Step: 100, Loss: 1.2290164232254028\n",
      "Epoch: 15, Validation Perplexity: 3.724700640232497\n",
      "To be, or not to be\n",
      "here off.\n",
      "\n",
      "MENENIUS:\n",
      "Let's have assistance.\n",
      "\n",
      "ABHORSON:\n",
      "The business shall have mouth'd in my world;\n",
      "Epoch: 16/20, Step: 0, Loss: 1.2835978269577026\n",
      "Epoch: 16/20, Step: 100, Loss: 1.2253131866455078\n",
      "Epoch: 16, Validation Perplexity: 3.729927680278464\n",
      "To be, or not to be love's.\n",
      "\n",
      "SLY:\n",
      "It be jount; which you shall posted you\n",
      "Of close in Angelo!\n",
      "\n",
      "STANLEY:\n",
      "My mild ombope!\n",
      "Epoch: 17/20, Step: 0, Loss: 1.2891758680343628\n",
      "Epoch: 17/20, Step: 100, Loss: 1.2176569700241089\n",
      "Epoch: 17, Validation Perplexity: 3.7405709755101704\n",
      "To be, or not to be\n",
      "In friend and Clerumpetbing of it.\n",
      "\n",
      "PRINCE:\n",
      "Receive you to be Claudio, thus I'll pray\n",
      "His lightly: \n",
      "Epoch: 18/20, Step: 0, Loss: 1.2872841358184814\n",
      "Epoch: 18/20, Step: 100, Loss: 1.2200981378555298\n",
      "Epoch: 18, Validation Perplexity: 3.743280894004315\n",
      "To be, or not to be come:\n",
      "For this be for our desires to Northumberland.\n",
      "\n",
      "RIVERS:\n",
      "Ay, sir; for day I have other line fo\n",
      "Epoch: 19/20, Step: 0, Loss: 1.2862794399261475\n",
      "Epoch: 19/20, Step: 100, Loss: 1.2230744361877441\n",
      "Epoch: 19, Validation Perplexity: 3.761175650617499\n",
      "To be, or not to be, though\n",
      "then sady broke and tuning to beght thou flowed.\n",
      "Thou, in his queen, you think it is now a \n",
      "Epoch: 20/20, Step: 0, Loss: 1.2780834436416626\n",
      "Epoch: 20/20, Step: 100, Loss: 1.2092242240905762\n",
      "Epoch: 20, Validation Perplexity: 3.7663122110322784\n",
      "To be, or not to be,\n",
      "But eagles in his eyes of one to villien hand.\n",
      "Now, come realm, and doth be touchest not so lie.\n",
      "H\n"
     ]
    }
   ],
   "source": [
    "# split the dataset into training and validation sets\n",
    "split_index = int(len(encoded_text) * 0.9)\n",
    "train_data = encoded_text[:split_index]\n",
    "val_data = encoded_text[split_index:]\n",
    "\n",
    "# train and evaluate the model\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    hidden = tuple([each.data.to(device) for each in hidden])\n",
    "\n",
    "    for i, (x, y) in enumerate(get_batches(train_data, batch_size, seq_length)):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(x, hidden)\n",
    "        loss = criterion(output, y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Step: {i}, Loss: {loss.item()}')\n",
    "\n",
    "    # evaluate on validation set\n",
    "    val_perplexity = calculate_perplexity(model, val_data, seq_length, batch_size)\n",
    "    print(f'Epoch: {epoch+1}, Validation Perplexity: {val_perplexity}')\n",
    "    print(generate_text(model, start_str=\"To be, or not to be\", length=100, hidden_size=hidden_size, num_layers=num_layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing probabilities and preplexities (side-by-side)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(model, data, seq_length, batch_size):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    hidden = tuple([each.data.to(device) for each in hidden])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output, hidden = model(x, hidden)\n",
    "            loss = criterion(output, y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / (len(data) / (batch_size * seq_length))\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return perplexity\n",
    "\n",
    "def print_text_with_probabilities_and_perplexity(model, start_str, length, hidden_size, num_layers):\n",
    "    model.eval()\n",
    "    chars = [char_to_idx[ch] for ch in start_str]\n",
    "    input = torch.tensor(chars, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    hidden = model.init_hidden(1)\n",
    "    hidden = tuple([each.data.to(device) for each in hidden])\n",
    "\n",
    "    generated_text = start_str\n",
    "    total_log_prob = 0\n",
    "    for i in range(length):\n",
    "        output, hidden = model(input, hidden)\n",
    "        p = torch.nn.functional.softmax(output, dim=1).data\n",
    "        top_ch = torch.multinomial(p, 1)[-1]\n",
    "        char = idx_to_char[top_ch.item()]\n",
    "        char_probability = p[0][top_ch].item()\n",
    "        total_log_prob += math.log(char_probability)\n",
    "        generated_text += char\n",
    "        print(f\"Character: {char}, Probability: {char_probability:.4f}\")\n",
    "        input = torch.tensor([top_ch], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    avg_log_prob = total_log_prob / length\n",
    "    perplexity = math.exp(-avg_log_prob)\n",
    "    print(f\"Generated Text Perplexity: {perplexity:.4f}\")\n",
    "    return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character: ., Probability: 0.0007\n",
      "Character: \n",
      ", Probability: 0.9652\n",
      "Character: \n",
      ", Probability: 0.6088\n",
      "Character: H, Probability: 0.0428\n",
      "Character: A, Probability: 0.2716\n",
      "Character: S, Probability: 0.9971\n",
      "Character: T, Probability: 0.9999\n",
      "Character: I, Probability: 1.0000\n",
      "Character: N, Probability: 1.0000\n",
      "Character: G, Probability: 0.9994\n",
      "Character: S, Probability: 1.0000\n",
      "Character: :, Probability: 0.9999\n",
      "Character: \n",
      ", Probability: 0.9999\n",
      "Character: I, Probability: 0.1647\n",
      "Character:  , Probability: 0.5984\n",
      "Character: w, Probability: 0.1994\n",
      "Character: i, Probability: 0.5728\n",
      "Character: l, Probability: 0.9460\n",
      "Character: l, Probability: 0.9993\n",
      "Character:  , Probability: 0.9091\n",
      "Character: a, Probability: 0.0262\n",
      "Character: s, Probability: 0.0926\n",
      "Character: s, Probability: 0.2914\n",
      "Character: a, Probability: 0.1474\n",
      "Character: m, Probability: 0.0328\n",
      "Character: e, Probability: 0.9970\n",
      "Character:  , Probability: 0.9201\n",
      "Character: t, Probability: 0.3112\n",
      "Character: o, Probability: 0.5220\n",
      "Character:  , Probability: 0.8799\n",
      "Character: t, Probability: 0.2380\n",
      "Character: h, Probability: 0.9434\n",
      "Character: e, Probability: 0.7457\n",
      "Character:  , Probability: 0.7913\n",
      "Character: e, Probability: 0.0308\n",
      "Character: x, Probability: 0.2129\n",
      "Character: i, Probability: 0.0400\n",
      "Character: l, Probability: 0.8876\n",
      "Character: e, Probability: 0.9692\n",
      "Character:  , Probability: 0.5731\n",
      "Character: t, Probability: 0.1620\n",
      "Character: h, Probability: 0.4185\n",
      "Character: o, Probability: 0.0527\n",
      "Character: u, Probability: 0.9675\n",
      "Character:  , Probability: 0.8572\n",
      "Character: s, Probability: 0.1464\n",
      "Character: h, Probability: 0.6549\n",
      "Character: o, Probability: 0.1800\n",
      "Character: u, Probability: 0.9109\n",
      "Character: l, Probability: 0.9974\n",
      "Character: d, Probability: 0.9996\n",
      "Character:  , Probability: 0.1605\n",
      "Character: b, Probability: 0.2086\n",
      "Character: e, Probability: 0.8942\n",
      "Character:  , Probability: 0.5019\n",
      "Character: m, Probability: 0.0524\n",
      "Character: o, Probability: 0.2288\n",
      "Character: s, Probability: 0.1717\n",
      "Character: t, Probability: 0.9973\n",
      "Character: \n",
      ", Probability: 0.2259\n",
      "Character: h, Probability: 0.0638\n",
      "Character: a, Probability: 0.2625\n",
      "Character: i, Probability: 0.0238\n",
      "Character: r, Probability: 0.2325\n",
      "Character:  , Probability: 0.7455\n",
      "Character: f, Probability: 0.0769\n",
      "Character: o, Probability: 0.6828\n",
      "Character: r, Probability: 0.9423\n",
      "Character:  , Probability: 0.9036\n",
      "Character: h, Probability: 0.0812\n",
      "Character: i, Probability: 0.7527\n",
      "Character: s, Probability: 0.6871\n",
      "Character:  , Probability: 0.9682\n",
      "Character: d, Probability: 0.0489\n",
      "Character: e, Probability: 0.4826\n",
      "Character: a, Probability: 0.4513\n",
      "Character: t, Probability: 0.5724\n",
      "Character: h, Probability: 0.9995\n",
      "Character: ,, Probability: 0.1829\n",
      "Character:  , Probability: 0.9341\n",
      "Character: a, Probability: 0.2702\n",
      "Character: s, Probability: 0.0525\n",
      "Character:  , Probability: 0.9629\n",
      "Character: w, Probability: 0.0767\n",
      "Character: e, Probability: 0.6887\n",
      "Character:  , Probability: 0.7880\n",
      "Character: r, Probability: 0.0181\n",
      "Character: e, Probability: 0.7329\n",
      "Character: m, Probability: 0.3024\n",
      "Character: a, Probability: 0.3771\n",
      "Character: i, Probability: 0.8863\n",
      "Character: n, Probability: 0.9670\n",
      "Character:  , Probability: 0.3629\n",
      "Character: t, Probability: 0.3000\n",
      "Character: o, Probability: 0.5254\n",
      "Character: ', Probability: 0.0059\n",
      "Character: t, Probability: 0.9264\n",
      "Character: ., Probability: 0.3036\n",
      "Character: \n",
      ", Probability: 0.9109\n",
      "Character: \n",
      ", Probability: 0.9482\n",
      "Generated Text Perplexity: 3.0241\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"To be, or not to be.\\n\\nHASTINGS:\\nI will assame to the exile thou should be most\\nhair for his death, as we remain to't.\\n\\n\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_text_with_probabilities_and_perplexity(model, start_str=\"To be, or not to be\", \n",
    "                                             length=100, hidden_size=hidden_size, num_layers=num_layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: using GPT-2 model from the transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aa9aeeba151448ba16b544cd5963a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76585f6fe904256a9c2ed819f221a68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33d1a713f3284478ae1bc9a20d74ae46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "231fe8413bf24751be9d9005a704dfd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9fae861fcc4708aa5519ac69ad5520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31d95f7ae61140d2a10e6879af06b1dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load pre-trained model and tokenizer\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate text using GPT-2\n",
    "def generate_text_gpt2(model, tokenizer, start_str, length):\n",
    "    # Encode the input text\n",
    "    input_ids = tokenizer.encode(start_str, return_tensors='pt')\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_length=length, temperature=0.7, top_k=50, top_p=0.9, num_return_sequences=1)\n",
    "    \n",
    "    # Decode the generated text\n",
    "    generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sheidamajidi/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:362: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/Users/sheidamajidi/anaconda3/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:367: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be, or not to be, a member of the Church of Jesus Christ of Latter-day Saints, I am not a member of the Church of Jesus Christ of Latter-day Saints. I am not a member of the Church of Jesus Christ of Latter-day Saints. I am not a member of the Church of Jesus Christ of Latter-day Saints. I am not a member of the Church of Jesus Christ of Latter-day Saints. I am not a member of the Church of Jesus Christ of Latter-day Saints. I am not a member of the Church of Jesus Christ of Latter-day Saints. I am not a member of the Church of Jesus Christ of Latter-day Saints. I am not a member of the Church of Jesus Christ of Latter-day Saints. I am not a member of the Church of Jesus Christ of Latter-day Saints. I am not a member of the Church of Jesus Christ of Latter-day Saints. I am not a member of the Church\n"
     ]
    }
   ],
   "source": [
    "# generate text with GPT-2\n",
    "start_str = \"To be, or not to be\"\n",
    "generated_text = generate_text_gpt2(model, tokenizer, start_str, length=200)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate perplexity for the generated text\n",
    "def calculate_perplexity_gpt2(model, tokenizer, text):\n",
    "    input_ids = tokenizer.encode(text, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        perplexity = torch.exp(loss).item()\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 18.4079\n"
     ]
    }
   ],
   "source": [
    "example_text = \"To be, or not to be, that is the question.\"\n",
    "perplexity = calculate_perplexity_gpt2(model, tokenizer, example_text)\n",
    "print(f\"Perplexity: {perplexity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experiment 2 : Hyper-parameter tuning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(set(text))\n",
    "char_to_idx = {ch: i for i, ch in enumerate(chars)}\n",
    "idx_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "\n",
    "encoded_text = np.array([char_to_idx[ch] for ch in text])\n",
    "\n",
    "seq_length = 100\n",
    "batch_size = 64\n",
    "\n",
    "def get_batches(encoded_text, batch_size, seq_length):\n",
    "    total_length = len(encoded_text)\n",
    "    num_batches = total_length // (batch_size * seq_length)\n",
    "\n",
    "    encoded_text = encoded_text[:num_batches * batch_size * seq_length]\n",
    "    encoded_text = encoded_text.reshape((batch_size, -1))\n",
    "\n",
    "    for i in range(0, encoded_text.shape[1], seq_length):\n",
    "        x = encoded_text[:, i:i+seq_length]\n",
    "        y = np.zeros_like(x)\n",
    "        if i+seq_length < encoded_text.shape[1]:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], encoded_text[:, i+seq_length]\n",
    "        else:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], encoded_text[:, 0]\n",
    "        yield torch.tensor(x, dtype=torch.long), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, num_layers, dropout=0.5):\n",
    "        super(CharRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        out = out.reshape(out.size(0)*out.size(1), self.hidden_size)\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        return (weight.new(self.num_layers, batch_size, self.hidden_size).zero_(),\n",
    "                weight.new(self.num_layers, batch_size, self.hidden_size).zero_())\n",
    "\n",
    "def calculate_perplexity(model, data, seq_length, batch_size, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    hidden = tuple([each.data.to(device) for each in hidden])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            output, hidden = model(x, hidden)\n",
    "            loss = criterion(output, y.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / (len(data) / (batch_size * seq_length))\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameter optimization function\n",
    "def objective(trial):\n",
    "    hidden_size = trial.suggest_int('hidden_size', 128, 512)\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 4)\n",
    "    dropout = trial.suggest_float('dropout', 0.2, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # model, optimizer, and loss function\n",
    "    model = CharRNN(len(chars), hidden_size, num_layers, dropout).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    split_index = int(len(encoded_text) * 0.9)\n",
    "    train_data = encoded_text[:split_index]\n",
    "    val_data = encoded_text[split_index:]\n",
    "\n",
    "    num_epochs = 3 \n",
    "    batch_size = 32  \n",
    "\n",
    "    # training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        hidden = tuple([each.data.to(device) for each in hidden])\n",
    "\n",
    "        for i, (x, y) in enumerate(get_batches(train_data, batch_size, seq_length)):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            hidden = tuple([each.data for each in hidden])\n",
    "            model.zero_grad()\n",
    "            output, hidden = model(x, hidden)\n",
    "            loss = criterion(output, y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    val_perplexity = calculate_perplexity(model, val_data, seq_length, batch_size, device)\n",
    "    return val_perplexity\n",
    "\n",
    "split_index = int(len(encoded_text) * 0.9)\n",
    "train_data = encoded_text[:split_index]\n",
    "val_data = encoded_text[split_index:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-24 20:37:57,004] A new study created in memory with name: no-name-b6f13f6d-400c-4ce9-ae7f-a79593e70e9b\n",
      "[I 2024-05-24 20:42:18,783] Trial 0 finished with value: 5.686259741489859 and parameters: {'hidden_size': 301, 'num_layers': 3, 'dropout': 0.4130703337189342, 'learning_rate': 0.0007312102034508422}. Best is trial 0 with value: 5.686259741489859.\n",
      "/Users/sheidamajidi/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.32888205280417027 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "[I 2024-05-24 20:43:46,536] Trial 1 finished with value: 4.777773780099476 and parameters: {'hidden_size': 289, 'num_layers': 1, 'dropout': 0.32888205280417027, 'learning_rate': 0.006239549852964147}. Best is trial 1 with value: 4.777773780099476.\n",
      "/Users/sheidamajidi/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.41065148460888434 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "[I 2024-05-24 20:45:57,327] Trial 2 finished with value: 4.615608849778578 and parameters: {'hidden_size': 443, 'num_layers': 1, 'dropout': 0.41065148460888434, 'learning_rate': 0.001490822769789604}. Best is trial 2 with value: 4.615608849778578.\n",
      "[I 2024-05-24 20:47:50,449] Trial 3 finished with value: 5.785774543500441 and parameters: {'hidden_size': 172, 'num_layers': 3, 'dropout': 0.2483205210031858, 'learning_rate': 0.00183134104107531}. Best is trial 2 with value: 4.615608849778578.\n",
      "/Users/sheidamajidi/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2782016067423252 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "[I 2024-05-24 20:49:41,814] Trial 4 finished with value: 4.665666871509926 and parameters: {'hidden_size': 365, 'num_layers': 1, 'dropout': 0.2782016067423252, 'learning_rate': 0.004821612071649887}. Best is trial 2 with value: 4.615608849778578.\n",
      "[I 2024-05-24 20:57:01,209] Trial 5 finished with value: 4.8386197772921475 and parameters: {'hidden_size': 487, 'num_layers': 3, 'dropout': 0.25705605226929484, 'learning_rate': 0.005508633646208713}. Best is trial 2 with value: 4.615608849778578.\n",
      "[I 2024-05-24 20:58:18,065] Trial 6 finished with value: 4.9328979525340575 and parameters: {'hidden_size': 160, 'num_layers': 2, 'dropout': 0.29363983465933624, 'learning_rate': 0.008913717820631082}. Best is trial 2 with value: 4.615608849778578.\n",
      "[I 2024-05-24 21:14:43,184] Trial 7 finished with value: 5.200312773527825 and parameters: {'hidden_size': 425, 'num_layers': 3, 'dropout': 0.24692048073397158, 'learning_rate': 0.004546780405787045}. Best is trial 2 with value: 4.615608849778578.\n",
      "[I 2024-05-24 21:16:11,906] Trial 8 finished with value: 4.94775130066217 and parameters: {'hidden_size': 204, 'num_layers': 2, 'dropout': 0.40817627328407113, 'learning_rate': 0.0067804389608644485}. Best is trial 2 with value: 4.615608849778578.\n",
      "/Users/sheidamajidi/anaconda3/lib/python3.11/site-packages/torch/nn/modules/rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.41189408820313966 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "[I 2024-05-24 21:19:29,669] Trial 9 finished with value: 4.629433746300318 and parameters: {'hidden_size': 398, 'num_layers': 1, 'dropout': 0.41189408820313966, 'learning_rate': 0.003527984882417114}. Best is trial 2 with value: 4.615608849778578.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_size': 443, 'num_layers': 1, 'dropout': 0.41065148460888434, 'learning_rate': 0.001490822769789604}\n"
     ]
    }
   ],
   "source": [
    "# optimize hyperparameters using Optuna\n",
    "try:\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=10) \n",
    "\n",
    "    # best hyperparameters\n",
    "    print(f'Best hyperparameters: {study.best_params}')\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during the hyperparameter optimization: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train final model with best hyperparameters\n",
    "best_hidden_size = study.best_params['hidden_size']\n",
    "best_num_layers = study.best_params['num_layers']\n",
    "best_dropout = study.best_params['dropout']\n",
    "best_learning_rate = study.best_params['learning_rate']\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = CharRNN(len(chars), best_hidden_size, best_num_layers, best_dropout).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20, Step: 0, Loss: 4.1894917488098145\n",
      "Epoch: 1/20, Step: 100, Loss: 1.8075377941131592\n",
      "Epoch: 1, Validation Perplexity: 5.813567866657465\n",
      "Epoch: 2/20, Step: 0, Loss: 1.683000922203064\n",
      "Epoch: 2/20, Step: 100, Loss: 1.514767050743103\n",
      "Epoch: 2, Validation Perplexity: 5.057815673916474\n",
      "Epoch: 3/20, Step: 0, Loss: 1.5278393030166626\n",
      "Epoch: 3/20, Step: 100, Loss: 1.4161667823791504\n",
      "Epoch: 3, Validation Perplexity: 4.754670684331333\n",
      "Epoch: 4/20, Step: 0, Loss: 1.459116816520691\n",
      "Epoch: 4/20, Step: 100, Loss: 1.3566111326217651\n",
      "Epoch: 4, Validation Perplexity: 4.600151532055493\n",
      "Epoch: 5/20, Step: 0, Loss: 1.417265772819519\n",
      "Epoch: 5/20, Step: 100, Loss: 1.315069556236267\n",
      "Epoch: 5, Validation Perplexity: 4.503432373073049\n",
      "Epoch: 6/20, Step: 0, Loss: 1.3874467611312866\n",
      "Epoch: 6/20, Step: 100, Loss: 1.286935567855835\n",
      "Epoch: 6, Validation Perplexity: 4.446841592578943\n",
      "Epoch: 7/20, Step: 0, Loss: 1.3609236478805542\n",
      "Epoch: 7/20, Step: 100, Loss: 1.2627443075180054\n",
      "Epoch: 7, Validation Perplexity: 4.415107935295037\n",
      "Epoch: 8/20, Step: 0, Loss: 1.3399887084960938\n",
      "Epoch: 8/20, Step: 100, Loss: 1.2443386316299438\n",
      "Epoch: 8, Validation Perplexity: 4.405723109302838\n",
      "Epoch: 9/20, Step: 0, Loss: 1.323488473892212\n",
      "Epoch: 9/20, Step: 100, Loss: 1.2285857200622559\n",
      "Epoch: 9, Validation Perplexity: 4.400766038902203\n",
      "Epoch: 10/20, Step: 0, Loss: 1.2995816469192505\n",
      "Epoch: 10/20, Step: 100, Loss: 1.2094922065734863\n",
      "Epoch: 10, Validation Perplexity: 4.390809818339687\n",
      "Epoch: 11/20, Step: 0, Loss: 1.2842336893081665\n",
      "Epoch: 11/20, Step: 100, Loss: 1.1955287456512451\n",
      "Epoch: 11, Validation Perplexity: 4.3878971575476955\n",
      "Epoch: 12/20, Step: 0, Loss: 1.2715984582901\n",
      "Epoch: 12/20, Step: 100, Loss: 1.180469036102295\n",
      "Epoch: 12, Validation Perplexity: 4.386112650846518\n",
      "Epoch: 13/20, Step: 0, Loss: 1.2574012279510498\n",
      "Epoch: 13/20, Step: 100, Loss: 1.1704821586608887\n",
      "Epoch: 13, Validation Perplexity: 4.401017423463142\n",
      "Epoch: 14/20, Step: 0, Loss: 1.2469863891601562\n",
      "Epoch: 14/20, Step: 100, Loss: 1.1610405445098877\n",
      "Epoch: 14, Validation Perplexity: 4.422331474880958\n",
      "Epoch: 15/20, Step: 0, Loss: 1.2343413829803467\n",
      "Epoch: 15/20, Step: 100, Loss: 1.1459635496139526\n",
      "Epoch: 15, Validation Perplexity: 4.430805546878139\n",
      "Epoch: 16/20, Step: 0, Loss: 1.2242536544799805\n",
      "Epoch: 16/20, Step: 100, Loss: 1.1417328119277954\n",
      "Epoch: 16, Validation Perplexity: 4.419834733463977\n",
      "Epoch: 17/20, Step: 0, Loss: 1.2123931646347046\n",
      "Epoch: 17/20, Step: 100, Loss: 1.125718116760254\n",
      "Epoch: 17, Validation Perplexity: 4.435674029534583\n",
      "Epoch: 18/20, Step: 0, Loss: 1.199865460395813\n",
      "Epoch: 18/20, Step: 100, Loss: 1.112342119216919\n",
      "Epoch: 18, Validation Perplexity: 4.467651307284097\n",
      "Epoch: 19/20, Step: 0, Loss: 1.191611647605896\n",
      "Epoch: 19/20, Step: 100, Loss: 1.1044211387634277\n",
      "Epoch: 19, Validation Perplexity: 4.486190218486199\n",
      "Epoch: 20/20, Step: 0, Loss: 1.1842305660247803\n",
      "Epoch: 20/20, Step: 100, Loss: 1.0963201522827148\n",
      "Epoch: 20, Validation Perplexity: 4.508873221086948\n"
     ]
    }
   ],
   "source": [
    "# training loop for final model\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    hidden = tuple([each.data.to(device) for each in hidden])\n",
    "\n",
    "    for i, (x, y) in enumerate(get_batches(train_data, batch_size, seq_length)):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        hidden = tuple([each.data for each in hidden])\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(x, hidden)\n",
    "        loss = criterion(output, y.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'Epoch: {epoch+1}/{num_epochs}, Step: {i}, Loss: {loss.item()}')\n",
    "\n",
    "    # evaluate on validation set\n",
    "    val_perplexity = calculate_perplexity(model, val_data, seq_length, batch_size, device)\n",
    "    print(f'Epoch: {epoch+1}, Validation Perplexity: {val_perplexity}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_text_with_probabilities_and_perplexity(model, start_str, length, hidden_size, num_layers, device):\n",
    "    model.eval()\n",
    "    chars = [char_to_idx[ch] for ch in start_str]\n",
    "    input = torch.tensor(chars, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    hidden = model.init_hidden(1)\n",
    "    hidden = tuple([each.data.to(device) for each in hidden])\n",
    "\n",
    "    generated_text = start_str\n",
    "    total_log_prob = 0\n",
    "    for i in range(length):\n",
    "        output, hidden = model(input, hidden)\n",
    "        p = torch.nn.functional.softmax(output, dim=1).data\n",
    "        top_ch = torch.multinomial(p, 1)[-1]\n",
    "        char = idx_to_char[top_ch.item()]\n",
    "        char_probability = p[0][top_ch].item()\n",
    "        total_log_prob += math.log(char_probability)\n",
    "        generated_text += char\n",
    "        print(f\"Character: {char}, Probability: {char_probability:.4f}\")\n",
    "        input = torch.tensor([top_ch], dtype=torch.long).unsqueeze(0).to(device)\n",
    "\n",
    "    avg_log_prob = total_log_prob / length\n",
    "    perplexity = math.exp(-avg_log_prob)\n",
    "    print(f\"Generated Text Perplexity: {perplexity:.4f}\")\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character:  , Probability: 0.0002\n",
      "Character: m, Probability: 0.0890\n",
      "Character: u, Probability: 0.0972\n",
      "Character: r, Probability: 0.1869\n",
      "Character: d, Probability: 0.9897\n",
      "Character: e, Probability: 0.9903\n",
      "Character: r, Probability: 0.9903\n",
      "Character: e, Probability: 0.5230\n",
      "Character: d, Probability: 0.5427\n",
      "Character: ,, Probability: 0.1741\n",
      "Character: \n",
      ", Probability: 0.8662\n",
      "Character: Y, Probability: 0.0363\n",
      "Character: o, Probability: 0.8318\n",
      "Character: u, Probability: 0.9888\n",
      "Character: n, Probability: 0.0208\n",
      "Character: g, Probability: 0.9904\n",
      "Character: s, Probability: 0.0334\n",
      "Character:  , Probability: 0.7315\n",
      "Character: a, Probability: 0.3643\n",
      "Character: n, Probability: 0.7184\n",
      "Character: d, Probability: 0.9921\n",
      "Character:  , Probability: 0.9980\n",
      "Character: s, Probability: 0.0683\n",
      "Character: i, Probability: 0.1281\n",
      "Character: t, Probability: 0.0368\n",
      "Character: t, Probability: 0.2653\n",
      "Character: i, Probability: 0.2837\n",
      "Character: n, Probability: 0.9974\n",
      "Character: g, Probability: 0.9665\n",
      "Character:  , Probability: 0.8366\n",
      "Character: f, Probability: 0.0593\n",
      "Character: a, Probability: 0.1278\n",
      "Character: i, Probability: 0.3084\n",
      "Character: r, Probability: 0.8005\n",
      "Character: -, Probability: 0.0044\n",
      "Character: l, Probability: 0.1546\n",
      "Character: u, Probability: 0.0118\n",
      "Character: s, Probability: 0.5724\n",
      "Character: t, Probability: 0.6310\n",
      "Character: o, Probability: 0.0609\n",
      "Character: r, Probability: 0.7002\n",
      "Character: s, Probability: 0.4692\n",
      "Character:  , Probability: 0.4801\n",
      "Character: w, Probability: 0.1284\n",
      "Character: i, Probability: 0.6426\n",
      "Character: l, Probability: 0.5197\n",
      "Character: l, Probability: 0.9963\n",
      "Character:  , Probability: 0.9080\n",
      "Character: s, Probability: 0.0649\n",
      "Character: h, Probability: 0.1380\n",
      "Character: a, Probability: 0.1200\n",
      "Character: m, Probability: 0.5608\n",
      "Character: e, Probability: 0.9950\n",
      "Character:  , Probability: 0.1422\n",
      "Character: s, Probability: 0.0501\n",
      "Character: i, Probability: 0.0396\n",
      "Character: n, Probability: 0.3375\n",
      "Character: c, Probability: 0.2283\n",
      "Character: e, Probability: 0.9902\n",
      "Character: ,, Probability: 0.1728\n",
      "Character: \n",
      ", Probability: 0.9914\n",
      "Character: A, Probability: 0.1507\n",
      "Character: n, Probability: 0.7349\n",
      "Character: d, Probability: 0.9920\n",
      "Character:  , Probability: 0.9521\n",
      "Character: h, Probability: 0.0519\n",
      "Character: e, Probability: 0.4786\n",
      "Character: a, Probability: 0.1565\n",
      "Character: r, Probability: 0.8682\n",
      "Character:  , Probability: 0.4799\n",
      "Character: h, Probability: 0.1404\n",
      "Character: e, Probability: 0.0812\n",
      "Character:  , Probability: 0.0940\n",
      "Character: d, Probability: 0.0299\n",
      "Character: i, Probability: 0.6355\n",
      "Character: s, Probability: 0.2806\n",
      "Character: t, Probability: 0.0898\n",
      "Character: r, Probability: 0.3804\n",
      "Character: e, Probability: 0.5451\n",
      "Character: s, Probability: 0.9667\n",
      "Character: s, Probability: 0.9892\n",
      "Character: e, Probability: 0.2780\n",
      "Character: d, Probability: 0.9388\n",
      "Character:  , Probability: 0.9096\n",
      "Character: w, Probability: 0.1853\n",
      "Character: i, Probability: 0.7822\n",
      "Character: t, Probability: 0.9376\n",
      "Character: h, Probability: 0.9811\n",
      "Character:  , Probability: 0.8680\n",
      "Character: t, Probability: 0.2025\n",
      "Character: r, Probability: 0.0221\n",
      "Character: o, Probability: 0.1125\n",
      "Character: u, Probability: 0.5060\n",
      "Character: b, Probability: 0.9474\n",
      "Character: l, Probability: 0.9892\n",
      "Character: e, Probability: 0.8239\n",
      "Character: ., Probability: 0.0563\n",
      "Character: \n",
      ", Probability: 0.9343\n",
      "Character: \n",
      ", Probability: 0.8592\n",
      "Character: C, Probability: 0.1084\n",
      "Generated Text Perplexity: 3.6802\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'To be, or not to be murdered,\\nYoungs and sitting fair-lustors will shame since,\\nAnd hear he distressed with trouble.\\n\\nC'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print_text_with_probabilities_and_perplexity(model, start_str=\"To be, or not to be\", \n",
    "                                             length=100, hidden_size=best_hidden_size, \n",
    "                                             num_layers=best_num_layers, device=device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (poenv)",
   "language": "python",
   "name": "poenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
